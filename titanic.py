# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iKIEME7OMQ5UuXN-umvOl049_v_fMDjE
"""

#Importing pandas to do import csv file and do preprocessing
import pandas as pd

data = pd.read_csv('train.csv')

data.shape

data.head()

data.columns

# To find unique values and also null values for loop have to be used
for col in data.columns:
  print(col, data[col].unique())

# Using for loop and value counts we are going to find how many times a particular value has repeated
for col in data.columns:
  print (col, data[col].value_counts())

# To find missing values we are going to use isnull command
data.isnull().sum()

# Now we are going to seperate dataset into feature matrix and target
y = data['Survived']
X = data.drop('Survived', axis = 1)

# To split the data into train and test data sklearn have to be imported
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split (X,y, test_size = 0.2, random_state = 1)

X_train.isnull().sum()
# 144b missing values are in age

#Since age has 144 missing values, we have to fill the values using fillna
X_train.Age = X_train.Age.fillna(X_train.Age.median())
X_test.Age = X_test.Age.fillna(X_train.Age.median())

# Embarked has 2 missing values , so fillna is used there also
X_train.Embarked = X_train.Embarked.fillna('S')
X_test.Embarked = X_test.Embarked.fillna('S')

X_train.isnull().sum()

# Passenger Id, Name, Ticket, Cabin are the columns which are not required for prediction.
cols_to_drop = ['PassengerId','Name','Ticket','Cabin']
X_train = X_train.drop(cols_to_drop, axis = 1)
X_test = X_test.drop(cols_to_drop, axis = 1)

# Since Sex and Embarked columns are in object data type, we use one hot encoder to make values categorical
from sklearn.preprocessing import OneHotEncoder
OHE = OneHotEncoder()
OHE_cols = OHE.fit_transform(X_train[['Sex','Embarked']])

OHE_cols.toarray()

# we have to convert array into data frame
OHE_cols = pd.DataFrame (OHE_cols.toarray(), columns = ['s1','s2','e1','e2','e3'])

#To concatenate two data frames, we have to use concat coammand.
X_train[['Pclass','Age','SibSp','Parch','Fare']]
X_train.reset_index(drop = True, inplace = True)
OHE_cols.reset_index(drop = True, inplace = True)
X_train = pd.concat([X_train[['Pclass','Age','SibSp','Parch','Fare']],OHE_cols],axis = 1)
X_train.head()

# Since it is test data, we use only transform.
OHE_col_test = OHE.transform (X_test[['Sex','Embarked']])
OHE_col_test = pd.DataFrame (OHE_col_test.toarray(), columns = ['s1','s2','e1','e2','e3'])
OHE_col_test

# After preprocessing we have to reset the index
X_test.reset_index(drop = True, inplace = True)
OHE_col_test.reset_index (drop = True, inplace = True)
X_test = pd.concat([X_test[['Pclass','Age','SibSp','Parch','Fare']], OHE_col_test],axis = 1)
X_test.head()

# To find the outlier, we use boxplot
import seaborn as sns
sns.boxplot(data = X_train)

# From boxplot we came to conclusion that the data has huge outliers. So we have to scale them using standard scaler
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
X_train = SS.fit_transform(X_train)
sns.boxplot(data = X_train)

X_test = SS.transform (X_test)

# Now we are going to apply different machine learning algorithms so that we can find which one has highest accuracy.
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression()
logreg.fit(X_train, y_train)
score_logreg = logreg.score(X_test, y_test)
score_logreg

"""K Nearest Neighbor"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors = 10)
knn.fit(X_train, y_train)
knn.score(X_test, y_test)

"""Support Vector Machine"""

from sklearn.svm import SVC
svc = SVC(random_state = 0)
from sklearn.model_selection import cross_val_score
cv_scores = cross_val_score(svc, X_train, y_train, cv=5)
cv_scores

cv_scores.mean()

cv_scores.std()

svc.fit(X_train, y_train)
svc.score(X_test, y_test)

"""Decsion Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(random_state = 0)
dtc.fit(X_train, y_train)
train_score = dtc.score(X_train, y_train)
test_score = dtc.score(X_test, y_test)

train_score

test_score

"""Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators = 100, max_depth = 2, random_state = 42)
rfc.fit(X_train, y_train)
rfc.score(X_test, y_test)

"""Gradient Boosting Classifier"""

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.1, random_state = 42)
gbc.fit(X_train,y_train)
gbc.score(X_test, y_test)